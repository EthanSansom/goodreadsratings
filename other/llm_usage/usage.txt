LLM: ChatGPT-4.
Date: 23 December 2025
Skip to content
Chat history


User said:
Here is the title and abstract of a paper I'm studying.

The Paradox of
Publicity: How Awards
Can Negatively Affect
the Evaluation of
Quality
Administrative Science Quarterly
59 (1)1â€“33
Ã“ The Author(s) 2014
Reprints and permissions:
sagepub.com/
journalsPermissions.nav
DOI: 10.1177/0001839214523602
asq.sagepub.com
Bala Â´ zs Kova Â´
cs1 and Amanda J. Sharkey2
Abstract
Although increases in status often lead to more favorable inferences about
quality in subsequent evaluations, in this paper, we examine a setting in which
an increase to an actorâ€™s status results in less favorable quality evaluations,
contrary to what much of sociological and management theory would predict.
Comparing thousands of reader reviews on Goodreads.com of 64 English-
language books that either won or were short-listed for prestigious book
awards between 2007 and 2011, we find that prizewinning books tend to
attract more readers following the announcement of an award and that readersâ€™
ratings of award-winning books tend to decline more precipitously following
the announcement of an award relative to books that were named as finalists
but did not win. We explain this surprising result, focusing on two mechanisms
whereby signals of quality that tend to promote adoption can subsequently
have a negative impact on evaluation. First, we propose that the audience eval-
uating a high-status actor or object tends to shift as a result of a public status
shock, like an award, increasing in number but also in diverse tastes. We out-
line how this shift might translate into less favorable evaluations of quality.
Second, we show that the increase in popularity that tends to follow a status
shock is off-putting to some, also resulting in more negative evaluations. We
show that our proposed mechanisms together explain the negative effect of
status on evaluations in the context of the literary world.
Keywords: status, awards, popularity, publicity, quality, Goodreads.com

Can you find me similar papers which deal with any/multiple of the following:
- Using Goodreads ratings as a research metric
- The challenges of using Goodreads ratings or other online user reviews to evaluate public perception
- The impact of awards on user ratings for novels or other materials (e.g. restaurants and michelin, albums and grammies, films and oscars, etc.)


ChatGPT said:
Here are academic papers, working papers, and studies that are similar in theme to the KovÃ¡cs & Sharkey (2014) paper youâ€™re studying â€” especially those dealing with Goodreads ratings, online user-review data, methodological challenges, and (where available) award effects on ratings/perceptions.

ğŸ“š Studies Using Goodreads Ratings or Online Book Reviews
A. Goodreads-Specific Research
Goodreads Reviews to Assess the Wider Impacts of Books
K. Kousha, M. Thelwall & M. Abdoli (2017)

Compares Goodreads book metrics (ratings, review counts) to other impact indicators (citations, library holdings) to gauge cultural/educational impact.

Explores limitations and how Goodreads data reflect reader engagement rather than scholarly quality. 
ResearchGate

Exploring Goodreads Reviews for Book Impact Assessment (Journal of Informetrics, 2019)

Analyses Goodreads reviews in terms of emotions, reviewer roles (librarians vs. ordinary users), popularity, etc.

Discusses challenges of interpreting online book reviews as impact indicators. 
ScienceDirect

The Goodreads â€œClassicsâ€: A Computational Study of Readers and Amateur Criticism (M. Walsh, 2021)

Computationally examines how Goodreads users define and discuss â€œclassicâ€ literature â€” useful for understanding how crowd definitions of quality emerge (and vary). 
Cultural Analytics

Investigating the Effects of Goodreads Challenges on Reading Habits (Jafari, Sabri & Bahrak, 2020)

Not about ratings per se, but dives into how Goodreads features shape user behavior â€” good for methodological context on using Goodreads engagement data. 
arXiv

The Use and Impact of Goodreads Rating and Reviews for Arabic Readers (AM Alghamdi)

Assesses how Goodreads influences book selection in a specific population â€” useful if you want cross-cultural/selection biases in Goodreads data. 
UWE Repository

ğŸ“Œ Note on methodology: Goodreads data are widely used in research precisely because they offer large-scale user engagement metrics, but several studies highlight challenges in interpreting them (e.g., reviewer roles, sentiment, bias), similar to your paperâ€™s concerns. 
ResearchGate

ğŸ“Š **Papers on Challenges of Using Online Ratings/Reviews
Even if not Goodreads-specific, these papers deal with issues highly relevant to your topic (online review bias, reliability, and evaluation challenges):

Ranking with Social Cues: Integrating Online Review Scores and Popularity

Shows how people weigh average ratings vs. popularity on retail/review platforms, highlighting complexities in interpreting online rating data. 
arXiv

Eliminating the Effect of Rating Bias on Reputation Systems

Addresses bias in online rating systems (Netflix, MovieLens), proposing adjustments to get closer to â€œtrueâ€ quality â€” relevant to the methodological challenges of average ratings. 
arXiv

When Sheep Shop: Measuring Herding Effects in Product Ratings (Lederrey & West, 2018)

Although not Goodreads-specific, this work shows how early ratings can skew later ones â€” a known issue in user-generated review datasets. 
arXiv

Grand Challenges in Altmetrics

Discusses validity and data-quality issues in online metrics â€” useful for conceptual framing if youâ€™re treating Goodreads ratings as a metric with limits. 
arXiv

ğŸ† Papers on Awards and Their Effects on Ratings/Perception
Directly Related to Award Impact Research
Impact of Academy Awards on Movie Reviews & Ratings (2024 working paper)

Examines how Oscars affect movie ratings and reviews before and after award announcements â€” conceptually similar to how literary awards may affect Goodreads ratings. 
Conscientia Beam

How the Academy Awards Influence Audience Perception

A thesis looking at whether Oscar nominations change audience perceptions of films â€” valuable if youâ€™re drawing parallels between different award domains. 
RIT Digital Repository

Film Awards as Indicators of Cinematic Creativity (Simonton, 2004)

Though older, this study treats awards (Oscars) as indicators of film quality and can be cited for broader context on award-rating relationships. 
eScholarship

ğŸ§  Complementary Literature on Online Reviews & Perception
These arenâ€™t Goodreads-specific, but they address rating/perception mechanisms you might find useful:

Chevalier & Mayzlin (2006): â€œThe Effect of Word of Mouth on Salesâ€ â€” classic study on Amazon/Nielsen reviews affecting consumer behavior (often cited in rating research literature). 
Mike Zhang

Studies on differential effects of online reviews on sales â€” explores how the source and type of review affects consumer decisions, a key methodological consideration when using online review data. 
PMC

ğŸ“ Why These Are Relevant
These papers collectively touch on the following themes youâ€™re interested in:

Topic	Key Papers
Goodreads as research metric	Kousha & Thelwall (2017); Wang et al. (2019); Walsh (2021)
Challenges of online review data	Lederrey & West (2018); Altmetrics critiques; reputation bias paper
Awardsâ€™ impact on ratings	Oscar impact studies (2024); related film award perception research
Methodological issues in review systems	Ranking with social cues; rating bias elimination
ğŸ“Œ How to Access These
Papers like â€œExploring Goodreads reviews for book impact assessmentâ€ are often behind paywalls â€” try your university/library subscription or ResearchGate preprints. 
ScienceDirect

Working papers and arXiv preprints (e.g., herding effects, Goodreads reading behavior) are usually freely accessible. 
arXiv
+1

If youâ€™d like, I can also pull PDF links/DOIs for these papers (where available) or a structured annotated bibliography you can use in your literature review!


User said:
Can you give me some more sources on the impact of awards on the perception of quality?


ChatGPT said:
Hereâ€™s a selection of academic and empirical research on how awards affect perceptions of quality or ratings, across books, movies, restaurants, music, and broader quality signals. These studies are useful for building your literature review or situating the Paradox of Publicity paper in a broader context.

ğŸ† Direct Studies on the Impact of Awards on Quality Perception
ğŸ”¹ 1. Creative Products and Awards
â€œWhat Wins Awards Is Not Always What I Buyâ€ â€” Journal of Consumer Research
Shows that artistic acclaim (e.g., award recognition) doesnâ€™t always translate into greater consumer liking or sales, across multiple domains (music, beer, etc.). It highlights dissociation between expert awards and consumer preferences. 
OUP Academic

ğŸ”¹ 2. Status and Evaluations in Movies
Chui (2024), Shifting evaluations: The impact of status increase on quality evaluations in the movie industry
Finds that Academy Award winners see more reviews and lower average ratings post-award than nominated-only films â€” very similar to the Goodreads/book finding in KovÃ¡cs & Sharkey but in film. 
Conscientia Beam

ğŸ¬ Film Awards, Signals & Perceptions
ğŸ”¹ 3. General Movie Award Effects
Research shows that major film awards (Oscars, Cannes, festival prizes) act as signals of quality and increase visibility and perceived quality, often translating into box office gains or elevated media attention. 
Number Analytics

ğŸ”¹ 4. Awards & Marketing/Signal Effectiveness
Studies indicate that effectiveness of quality signals depends on credibility and audience expectations â€” e.g., awards judged by experts influence consumer behavior more than crowd awards. 
Studeersnel

ğŸ½ï¸ Restaurant Awards (Michelin) and Consumer Perception
ğŸ”¹ 5. Michelin Star Effects on Consumer Values
A 2024 empirical study using OpenTable review data finds that receiving Michelin stars increases emotional/hedonic & social consumption values but often doesnâ€™t improve functional quality perceptions like food or ambience. 
ResearchGate

ğŸ”¹ 6. Sentiment & Online Ratings After Michelin Stars
Sentiment analysis of TripAdvisor reviews for Michelin-starred restaurants shows overall sentiment may decline post-award, arguably because of heightened expectations. 
ResearchGate

ğŸ”¹ 7. Consumer Perceptions of Michelin
Quantitative research finds that consumersâ€™ attitudes toward Michelin Guide ratings shape perceived risk and intentions â€” illustrating how awards act as third-party quality cues. 
ResearchGate

ğŸ”¹ 8. Expert Opinion vs. Consumer Ratings
A 2024 SSRN study using Michelin star changes shows that *decreases in expert rating can actually improve consumer review ratings, suggesting complex interactions between expert signals and consumer expectations. 
SSRN

ğŸ“Š Other Domains: Music, Science, and General Awards
ğŸ”¹ 9. Music Awards & Demand
A recorded music study finds that winning or being nominated for music awards has weak positive effects on chart performance, with variations by type of prize â€” useful if you want parallels to book awards. 
Erasmus University Thesis Repository

ğŸ”¹ 10. Scientific Prizes & Topic Growth
Research on scientific awards (Fields, Lasker, etc.) shows prizes can significantly boost research productivity and impact, illustrating awardsâ€™ differential long-term influence on perceived value in a domain. 
Nature

ğŸ”¹ 11. The Matthew Effect & Awards
Work on status and cumulative advantage (Matthew effect) shows that prestigious awards can significantly increase attention and subsequent perceived importance of recipientsâ€™ work â€” relevant for framing awards as quality signals. 
PMC

ğŸ§  Theoretical and Review Insights
ğŸ”¹ 12. Awards as Signals of Quality
Multiple literatures (e.g., signalling theory, cue utilization) show that awards serve as extrinsic cues that influence perceived quality and consumer trust, but effects vary by context and consumer expectations. 
ScienceDirect

ğŸ”¹ 13. Downsides of Awards
Popular accounts and syntheses note that winning awards can lead to backlash, higher expectations, and even a decline in perceived quality â€” aligning with the Goodreads findings. 
Booth School of Business

ğŸ“Œ Suggestions for Your Literature Review
To connect with your focal paper (KovÃ¡cs & Sharkey, 2014), you can cluster sources by theme:

A. Awards & User-Generated Ratings
Chui (movie IMDb ratings) 
Conscientia Beam

Michelin effect research (OpenTable) 
ResearchGate

Restaurant sentiment analyses 
ResearchGate

Michelin review dynamics (SSRN paper) 
SSRN

B. Awards & Quality Signals
Creative products vs consumer liking (JCR) 
OUP Academic

Film award signaling research 
Number Analytics

Consumer attitudes toward awards 
ResearchGate

C. Awards in Other Creative Industries
Music awards & charts 
Erasmus University Thesis Repository

Scientific awards & impact growth 
Nature

D. Theoretical Context
Signaling and reputation literature 
ScienceDirect

Downsides of status/recognition 
Booth School of Business

If youâ€™d like, I can also pull DOIs and PDF links (if available) for these sources, or help you draft annotated summaries for each for your literature review.


User said:
This paper deals with very few observations, since award winners are by definition rare. Can you find sources mentioning the challenges of conducting an analysis in this case. Preferably, the analysis would explicitly be of winners (and their qualities vs. other contenders), but can be about rare events more generally.


ChatGPT said:

Thought for 28s
Nice â€” good question. Below I pulled together methodological sources and practical approaches that directly address the problem of doing inference when the â€œtreatedâ€ group is tiny (e.g., award winners), and related issues for rare events and small-n causal inference. I grouped the sources by theme and added a one-line note about why each is useful for your literature review or methods section.

Key methodological papers (what to cite & why)
1) Rare-events / small-sample bias in binary models
King & Zeng (2001) â€” Logistic Regression in Rare Events Data â€” classic diagnosis and corrections for bias when events are rare; practical for any binary outcome (e.g., winner vs non-winner, rare reviews). 
Gking

Firth (1993) â€” Bias Reduction of Maximum Likelihood Estimates â€” introduces the penalized (Firth) likelihood that fixes separation and large bias in small samples; widely recommended for small-sample logit. 
OUP Academic

Applied notes & implementations (reviews / CRAN package): Rainey (2021) and the logistf package documentation describe when to use Firth and how it performs in practice. 
Cambridge University Press & Assessment
+1

Why this matters: if you model a binary outcome (e.g., whether a book wins) or analyze reviewer binary decisions with very few positives, standard MLE can be biased or fail; Firth/King-Zeng provide fixes. 
Gking
+1

2) Inference when there are few treated units (few winners)
Conley & Taber (2011) â€” Inference with Difference-in-Differences with a Small Number of Policy Changes â€” offers randomization-style inference tailored to very few treated groups. 
University of Wisconsin Users

Ferman & Pinto (2016/2021) â€” Inference in DiD with Few Treated Units â€” shows pitfalls of standard cluster inference and gives alternatives (permutation / randomization inference, adjustments for spatial correlation). 
FEA USP
+1

MacKinnon & Webb (2016/2020) â€” Randomization inference and bootstrap alternatives (practical guidance for DID & few treated clusters). 
Queen's University Economics
+1

Why this matters: when only a handful of units â€œreceive treatmentâ€ (e.g., only a few award winners in a sample), cluster-robust or asymptotic tests often over-reject; these papers offer validated inference alternatives (randomization/permutation tests, tailored bootstrap procedures). 
University of Wisconsin Users
+1

3) Comparative case / single-unit designs (synthetic control & case study inference)
Abadie, Diamond & Hainmueller (2009/2010) â€” Synthetic Control Methods for Comparative Case Studies â€” method designed for causal inference with one (or very few) treated units by constructing a synthetic counterfactual from many controls. Good when winners are essentially â€œcase studies.â€ 
NBER
+1

Reviews & practical guides (Bonander 2021; Urban Institute primer) â€” explains inference and when SCM is preferable to regression-based DID in small-treated situations. 
PMC
+1

Why this matters: if you want to analyze the effect of a single high-profile winner (or a handful) on outcomes (ratings, citations), synthetic control is a principled option to build a data-driven counterfactual. 
NBER
+1

4) Prize / winner case studies (Nobel and similar) â€” small-n practical examples
Mazloumian et al. (2011) â€” How citation boosts promote paradigm shifts and Nobel Prizes â€” analyzes citation changes around Nobel prizes and discusses methodological choices for sparse event inference. (Good exemplar of prize-effects research.) 
PLOS

Dong et al. (2023) â€” Nobel citation effects â€” uses case-study approaches and discusses exchangeability checks and matching for few treated units. 
ScienceDirect
+1

Why this matters: literature that studies Nobel/major prizes often faces exactly your problem (very few laureates), so the methods and sensitivity checks they use are directly transferable to book-award analyses. 
PLOS
+1

5) Randomization / permutation tests and approximate randomization
Canay, Romano & Shaikh (2017) â€” Randomization Tests under an Approximate Symmetry Assumption â€” theoretical basis for approximate randomization/permutation tests in clustered/limited settings (useful for few treated units). 
University of Chicago

Related applied work: literature on randomization inference for DID and approximate permutation tests (e.g., Carter et al., induced-order approaches) â€” good alternatives to asymptotic p-values when treated units are rare. 
Queen's University Economics
+1

Why this matters: when asymptotics fail (few treated), randomization / permutation inference often gives correct type-I error control, provided the assumptions match your design. 
University of Chicago

6) Bayesian hierarchical / small-area modeling (borrowing strength)
National Academies / small-area estimation reviews; Bayesian SAE papers â€” hierarchical/Bayesian models that â€œborrow strengthâ€ across units to stabilize estimates for rare events or very small subgroups. Useful if you want to estimate winnersâ€™ effects while sharing information across books/years. 
The National Academies Press
+1

Why this matters: Bayesian hierarchies provide principled shrinkage and better uncertainty quantification in small-n problems (tradeoff: stronger modelling assumptions). 
NCRM EPrints Repository

7) General reviews on analysis of rare events
LeitgÃ¶b (2019) â€” Analysis of Rare Events (SAGE Methods Foundations) â€” concise overview of the statistical challenges and choices (ML bias, separation, inference options). 
Academic Web Service

Short practical checklist (applies to award-winner analyses)
Donâ€™t rely on standard asymptotic standard errors if winners are few â€” cluster-robust or naive t's can be misleading. Consider randomization/permutation inference or tailored bootstrap. 
University of Wisconsin Users
+1

If you model binary rare outcomes, use bias-reduction (King-Zeng correction, or Firth penalized logit). 
Gking
+1

When you have 1â€“a few treated units across time, synthetic control may give more credible counterfactuals than pooled DID. 
NBER

Use randomization/permutation inference (Conleyâ€“Taber, Canayâ€“Romanoâ€“Shaikh, MacKinnon) to get valid p-values with few treated clusters. 
University of Wisconsin Users
+2
University of Chicago
+2

Consider Bayesian hierarchical models to borrow strength across books/years if youâ€™re willing to impose structure (helps precision, but check sensitivity). 
NCRM EPrints Repository

Run extensive robustness checks: leave-one-out treated unit checks, placebo tests on controls, pre-trend falsification, and exchangeability checks (used in Nobel studies). 
ScienceDirect
+1

Specific papers (copy-paste ready â€” with links I pulled)
King, G. & Zeng, L. (2001). Logistic Regression in Rare Events Data. (classic). 
Gking

Firth, D. (1993). Bias Reduction of Maximum Likelihood Estimates (Biometrika). 
OUP Academic

Conley, T. G. & Taber, C. R. (2011). Inference with Difference-in-Differences with a Small Number of Policy Changes (Review of Economics and Statistics). 
University of Wisconsin Users

Ferman, B. & Pinto, C. (2016/2021). Inference in Difference-in-Differences with Few Treated Units (working papers/AEJ versions). 
FEA USP
+1

Abadie, A., Diamond, A., & Hainmueller, J. (2010). Synthetic Control Methods for Comparative Case Studies (JASA/NBER paper). 
NBER
+1

Canay, I. A., Romano, J. P., & Shaikh, A. M. (2017). Randomization Tests under an Approximate Symmetry Assumption (Econometrica). 
University of Chicago

Mazloumian, A. et al. (2011). How citation boosts promote scientific paradigm shifts and Nobel Prizes (PLoS ONE) â€” prize case-study methods. 
PLOS

LeitgÃ¶b, H. (2019). Analysis of Rare Events (SAGE Research Methods Foundations). 
Academic Web Service

If you want, next I can (pick one):

A. Pull PDFs / DOIs / stable links for any subset of the above and prepare an annotated bibliography (1 paragraph per paper).

B. Draft a methods appendix for your paper describing which of these approaches you used (and why), with suggested robustness checks you should run on your Goodreads award analysis.

C. Try to find examples of book-award studies that explicitly discuss the â€œfew winnersâ€ problem (I found Nobel/movie/restaurant work â€” I can search more specifically for book-award methods papers).

Which of A/B/C should I do now?
